{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import scipy\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesRegressor\n",
    "import os\n",
    "from sklearn.metrics import matthews_corrcoef, recall_score, roc_auc_score, balanced_accuracy_score, roc_curve, auc, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import KNNImputer, IterativeImputer\n",
    "from sklearn import svm\n",
    "import xgboost\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pytorch_tabnet.tab_model import  TabNetClassifier\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'E:\\NNSience\\Dwarfs\\Brown-Dwarfs-NN\\data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.read_csv('data2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:, 'PS1imag':'W4mag']\n",
    "y = df.loc[:, 'label']\n",
    "cols = df.loc[:, 'PS1imag':'W4mag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = pandas.DataFrame(data=X, columns=cols.columns)\n",
    "dx['i_z'] = dx['PS1imag']-dx['PS1zmag']\n",
    "dx['i_y'] = dx['PS1imag']-dx['PS1ymag']\n",
    "dx['z_y'] = dx['PS1zmag']-dx['PS1ymag']\n",
    "dx['z_J'] = dx['PS1zmag']-dx['Jmag']\n",
    "dx['y_J'] = dx['PS1ymag']-dx['Jmag']\n",
    "dx['J_H'] = dx['Jmag']-dx['Hmag']\n",
    "dx['H_Ks'] = dx['Hmag']-dx['Ksmag']\n",
    "dx['W1_W2'] = dx['W1mag']-dx['W2mag']\n",
    "X_ = np.array(dx.loc[:, 'PS1imag':'W1_W2'])\n",
    "cols = dx.loc[:, 'PS1imag':'W1_W2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saser\\Anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py:638: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  \" reached.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "imputer = IterativeImputer(estimator=ExtraTreesRegressor(n_estimators=30, max_features=17, max_depth=20, min_samples_split=15), max_iter=50, random_state=123)\n",
    "X = imputer.fit_transform(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, x_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=123)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test of learning rules on L&T data sample with Carnero Rosell et al.(2019) and Burningham et al.(2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8731229062271024\n"
     ]
    }
   ],
   "source": [
    "pos = pandas.DataFrame(data=x_test, columns=cols.columns)\n",
    "mask =  (pos['z_y']>0.15) & (pos['i_z']>1.2) & (pos['y_J']>1.6) & (pos['PS1zmag']<22) # Carnero Rosell et al.(2019)\n",
    "mask =  (pos['z_J']>2.5) & (pos['Jmag']<18.8) # Burningham et al.(2013)\n",
    "corr_dr = matthews_corrcoef(mask, y_test)\n",
    "print(corr_dr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train2 = scaler.fit_transform(X_train)\n",
    "x_val2 = scaler.transform(x_val)\n",
    "x_test2 = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest, hyperparameters from optuna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9768605568917293"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=500, max_depth=29, min_samples_leaf=4, max_features= 4)\n",
    "rf_model.fit(X_train2, y_train)\n",
    "y_pred_rf = rf_model.predict(x_test2)\n",
    "matthews_corrcoef(y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM, hyperparameters from optuna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9616256886076121"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_model = svm.SVC(kernel='rbf', C=1.1785578339058878, class_weight='balanced', gamma='scale', coef0=4.37386719156764, decision_function_shape='ovr', random_state=123, probability=True)\n",
    "svc_model.fit(X_train2, y_train)\n",
    "y_pred_svc=svc_model.predict(x_test2)\n",
    "matthews_corrcoef(y_test, y_pred_svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost, hyperparameters from optuna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saser\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9741778180983233"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf = xgboost.XGBClassifier(max_depth=24, n_estimators=500, booster='gbtree', learning_rate=0.7643065952721012, gamma=0.60562070482283363, subsample=0.53759136820407905,\n",
    "                            n_jobs=2, random_state=1, verbosity=0)\n",
    "xgb_clf.fit(X_train2, y_train)\n",
    "y_pred_xgb = xgb_clf.predict(x_test2)\n",
    "matthews_corrcoef(y_pred_xgb, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx_train = pandas.DataFrame(X_train2)\n",
    "dx_train=dx_train.drop(columns=[0,1,2])\n",
    "dx_test = pandas.DataFrame(x_test2)\n",
    "dx_test=dx_train.drop(columns=[0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saser\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.963843605575871"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf2 = xgboost.XGBClassifier(max_depth=24, n_estimators=500, booster='gbtree', learning_rate=0.7643065952721012, gamma=0.60562070482283363, subsample=0.53759136820407905,\n",
    "                            n_jobs=2, random_state=1, verbosity=0)\n",
    "xgb_clf2.fit(dx_train, y_train)\n",
    "y_pred_xgb = xgb_clf2.predict(dx_test)\n",
    "matthews_corrcoef(y_pred_xgb, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabnet:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cpu\n",
      "epoch 0  | loss: 0.22962 | train_balanced_accuracy: 0.96239 | valid_balanced_accuracy: 0.96307 |  0:00:02s\n",
      "epoch 1  | loss: 0.06898 | train_balanced_accuracy: 0.96834 | valid_balanced_accuracy: 0.96931 |  0:00:04s\n",
      "epoch 2  | loss: 0.05854 | train_balanced_accuracy: 0.9758  | valid_balanced_accuracy: 0.98135 |  0:00:07s\n",
      "epoch 3  | loss: 0.06075 | train_balanced_accuracy: 0.97392 | valid_balanced_accuracy: 0.97814 |  0:00:09s\n",
      "epoch 4  | loss: 0.06892 | train_balanced_accuracy: 0.97841 | valid_balanced_accuracy: 0.98135 |  0:00:12s\n",
      "epoch 5  | loss: 0.0577  | train_balanced_accuracy: 0.97718 | valid_balanced_accuracy: 0.97814 |  0:00:14s\n",
      "epoch 6  | loss: 0.05059 | train_balanced_accuracy: 0.98369 | valid_balanced_accuracy: 0.98412 |  0:00:16s\n",
      "epoch 7  | loss: 0.06581 | train_balanced_accuracy: 0.97599 | valid_balanced_accuracy: 0.97814 |  0:00:18s\n",
      "epoch 8  | loss: 0.04319 | train_balanced_accuracy: 0.98231 | valid_balanced_accuracy: 0.98305 |  0:00:20s\n",
      "epoch 9  | loss: 0.06749 | train_balanced_accuracy: 0.97479 | valid_balanced_accuracy: 0.97814 |  0:00:22s\n",
      "epoch 10 | loss: 0.0671  | train_balanced_accuracy: 0.98247 | valid_balanced_accuracy: 0.97958 |  0:00:24s\n",
      "epoch 11 | loss: 0.06973 | train_balanced_accuracy: 0.98149 | valid_balanced_accuracy: 0.98154 |  0:00:26s\n",
      "epoch 12 | loss: 0.05168 | train_balanced_accuracy: 0.98212 | valid_balanced_accuracy: 0.98368 |  0:00:28s\n",
      "epoch 13 | loss: 0.05993 | train_balanced_accuracy: 0.98313 | valid_balanced_accuracy: 0.98323 |  0:00:29s\n",
      "epoch 14 | loss: 0.05317 | train_balanced_accuracy: 0.98196 | valid_balanced_accuracy: 0.98412 |  0:00:31s\n",
      "epoch 15 | loss: 0.05333 | train_balanced_accuracy: 0.9835  | valid_balanced_accuracy: 0.98109 |  0:00:33s\n",
      "epoch 16 | loss: 0.04951 | train_balanced_accuracy: 0.98265 | valid_balanced_accuracy: 0.98002 |  0:00:35s\n",
      "epoch 17 | loss: 0.04861 | train_balanced_accuracy: 0.98127 | valid_balanced_accuracy: 0.98198 |  0:00:37s\n",
      "epoch 18 | loss: 0.04463 | train_balanced_accuracy: 0.98265 | valid_balanced_accuracy: 0.98412 |  0:00:39s\n",
      "epoch 19 | loss: 0.04484 | train_balanced_accuracy: 0.9843  | valid_balanced_accuracy: 0.9843  |  0:00:41s\n",
      "epoch 20 | loss: 0.05124 | train_balanced_accuracy: 0.98265 | valid_balanced_accuracy: 0.98305 |  0:00:43s\n",
      "epoch 21 | loss: 0.05046 | train_balanced_accuracy: 0.98335 | valid_balanced_accuracy: 0.98002 |  0:00:45s\n",
      "epoch 22 | loss: 0.04539 | train_balanced_accuracy: 0.98345 | valid_balanced_accuracy: 0.98172 |  0:00:48s\n",
      "epoch 23 | loss: 0.05365 | train_balanced_accuracy: 0.98008 | valid_balanced_accuracy: 0.98047 |  0:00:50s\n",
      "epoch 24 | loss: 0.05854 | train_balanced_accuracy: 0.98252 | valid_balanced_accuracy: 0.98047 |  0:00:53s\n",
      "epoch 25 | loss: 0.05584 | train_balanced_accuracy: 0.98257 | valid_balanced_accuracy: 0.98065 |  0:00:55s\n",
      "epoch 26 | loss: 0.05112 | train_balanced_accuracy: 0.98345 | valid_balanced_accuracy: 0.98065 |  0:00:58s\n",
      "epoch 27 | loss: 0.05094 | train_balanced_accuracy: 0.98292 | valid_balanced_accuracy: 0.98065 |  0:00:59s\n",
      "epoch 28 | loss: 0.04981 | train_balanced_accuracy: 0.98404 | valid_balanced_accuracy: 0.98109 |  0:01:01s\n",
      "epoch 29 | loss: 0.04782 | train_balanced_accuracy: 0.98313 | valid_balanced_accuracy: 0.98065 |  0:01:03s\n",
      "epoch 30 | loss: 0.04727 | train_balanced_accuracy: 0.98223 | valid_balanced_accuracy: 0.98172 |  0:01:05s\n",
      "epoch 31 | loss: 0.04218 | train_balanced_accuracy: 0.98218 | valid_balanced_accuracy: 0.98305 |  0:01:08s\n",
      "epoch 32 | loss: 0.04913 | train_balanced_accuracy: 0.98411 | valid_balanced_accuracy: 0.98297 |  0:01:10s\n",
      "epoch 33 | loss: 0.04697 | train_balanced_accuracy: 0.98491 | valid_balanced_accuracy: 0.98519 |  0:01:12s\n",
      "epoch 34 | loss: 0.05012 | train_balanced_accuracy: 0.9852  | valid_balanced_accuracy: 0.98537 |  0:01:15s\n",
      "epoch 35 | loss: 0.04714 | train_balanced_accuracy: 0.98433 | valid_balanced_accuracy: 0.98582 |  0:01:18s\n",
      "epoch 36 | loss: 0.05039 | train_balanced_accuracy: 0.98305 | valid_balanced_accuracy: 0.98457 |  0:01:22s\n",
      "epoch 37 | loss: 0.04808 | train_balanced_accuracy: 0.98345 | valid_balanced_accuracy: 0.98279 |  0:01:25s\n",
      "epoch 38 | loss: 0.04773 | train_balanced_accuracy: 0.98345 | valid_balanced_accuracy: 0.9843  |  0:01:29s\n",
      "epoch 39 | loss: 0.04595 | train_balanced_accuracy: 0.98417 | valid_balanced_accuracy: 0.98475 |  0:01:35s\n",
      "epoch 40 | loss: 0.04126 | train_balanced_accuracy: 0.98483 | valid_balanced_accuracy: 0.98279 |  0:01:39s\n",
      "epoch 41 | loss: 0.04272 | train_balanced_accuracy: 0.98568 | valid_balanced_accuracy: 0.98234 |  0:01:43s\n",
      "epoch 42 | loss: 0.04891 | train_balanced_accuracy: 0.98523 | valid_balanced_accuracy: 0.98475 |  0:01:46s\n",
      "epoch 43 | loss: 0.05721 | train_balanced_accuracy: 0.98486 | valid_balanced_accuracy: 0.98323 |  0:01:49s\n",
      "epoch 44 | loss: 0.05118 | train_balanced_accuracy: 0.98539 | valid_balanced_accuracy: 0.98582 |  0:01:52s\n",
      "epoch 45 | loss: 0.04885 | train_balanced_accuracy: 0.98523 | valid_balanced_accuracy: 0.98733 |  0:01:54s\n",
      "epoch 46 | loss: 0.03828 | train_balanced_accuracy: 0.98542 | valid_balanced_accuracy: 0.98626 |  0:01:57s\n",
      "epoch 47 | loss: 0.04685 | train_balanced_accuracy: 0.98643 | valid_balanced_accuracy: 0.9843  |  0:01:59s\n",
      "epoch 48 | loss: 0.04696 | train_balanced_accuracy: 0.98552 | valid_balanced_accuracy: 0.98689 |  0:02:02s\n",
      "epoch 49 | loss: 0.04977 | train_balanced_accuracy: 0.98547 | valid_balanced_accuracy: 0.98493 |  0:02:04s\n",
      "epoch 50 | loss: 0.05473 | train_balanced_accuracy: 0.98438 | valid_balanced_accuracy: 0.9819  |  0:02:06s\n",
      "epoch 51 | loss: 0.04516 | train_balanced_accuracy: 0.98552 | valid_balanced_accuracy: 0.9843  |  0:02:08s\n",
      "epoch 52 | loss: 0.03981 | train_balanced_accuracy: 0.98435 | valid_balanced_accuracy: 0.98475 |  0:02:11s\n",
      "epoch 53 | loss: 0.05491 | train_balanced_accuracy: 0.98563 | valid_balanced_accuracy: 0.98341 |  0:02:13s\n",
      "epoch 54 | loss: 0.04509 | train_balanced_accuracy: 0.98558 | valid_balanced_accuracy: 0.98733 |  0:02:16s\n",
      "epoch 55 | loss: 0.0486  | train_balanced_accuracy: 0.98523 | valid_balanced_accuracy: 0.98519 |  0:02:19s\n",
      "epoch 56 | loss: 0.04481 | train_balanced_accuracy: 0.98536 | valid_balanced_accuracy: 0.98475 |  0:02:21s\n",
      "epoch 57 | loss: 0.04272 | train_balanced_accuracy: 0.98552 | valid_balanced_accuracy: 0.98537 |  0:02:24s\n",
      "epoch 58 | loss: 0.04678 | train_balanced_accuracy: 0.98552 | valid_balanced_accuracy: 0.98386 |  0:02:26s\n",
      "epoch 59 | loss: 0.03793 | train_balanced_accuracy: 0.98536 | valid_balanced_accuracy: 0.98279 |  0:02:29s\n",
      "epoch 60 | loss: 0.03638 | train_balanced_accuracy: 0.98534 | valid_balanced_accuracy: 0.98493 |  0:02:32s\n",
      "epoch 61 | loss: 0.04507 | train_balanced_accuracy: 0.98621 | valid_balanced_accuracy: 0.98386 |  0:02:35s\n",
      "epoch 62 | loss: 0.03167 | train_balanced_accuracy: 0.9851  | valid_balanced_accuracy: 0.98412 |  0:02:38s\n",
      "epoch 63 | loss: 0.0329  | train_balanced_accuracy: 0.98491 | valid_balanced_accuracy: 0.98261 |  0:02:40s\n",
      "epoch 64 | loss: 0.03708 | train_balanced_accuracy: 0.98491 | valid_balanced_accuracy: 0.98216 |  0:02:43s\n",
      "epoch 65 | loss: 0.03922 | train_balanced_accuracy: 0.98579 | valid_balanced_accuracy: 0.98412 |  0:02:45s\n",
      "epoch 66 | loss: 0.03312 | train_balanced_accuracy: 0.98526 | valid_balanced_accuracy: 0.98519 |  0:02:46s\n",
      "epoch 67 | loss: 0.03156 | train_balanced_accuracy: 0.98746 | valid_balanced_accuracy: 0.98493 |  0:02:48s\n",
      "epoch 68 | loss: 0.03953 | train_balanced_accuracy: 0.98722 | valid_balanced_accuracy: 0.98493 |  0:02:50s\n",
      "epoch 69 | loss: 0.04238 | train_balanced_accuracy: 0.9839  | valid_balanced_accuracy: 0.98198 |  0:02:52s\n",
      "epoch 70 | loss: 0.03782 | train_balanced_accuracy: 0.98558 | valid_balanced_accuracy: 0.98582 |  0:02:54s\n",
      "epoch 71 | loss: 0.04241 | train_balanced_accuracy: 0.98558 | valid_balanced_accuracy: 0.98475 |  0:02:56s\n",
      "epoch 72 | loss: 0.03755 | train_balanced_accuracy: 0.98674 | valid_balanced_accuracy: 0.98386 |  0:02:58s\n",
      "epoch 73 | loss: 0.03703 | train_balanced_accuracy: 0.98677 | valid_balanced_accuracy: 0.9802  |  0:03:00s\n",
      "epoch 74 | loss: 0.03982 | train_balanced_accuracy: 0.98589 | valid_balanced_accuracy: 0.98279 |  0:03:02s\n",
      "epoch 75 | loss: 0.03675 | train_balanced_accuracy: 0.98592 | valid_balanced_accuracy: 0.9843  |  0:03:04s\n",
      "epoch 76 | loss: 0.04029 | train_balanced_accuracy: 0.98592 | valid_balanced_accuracy: 0.9843  |  0:03:06s\n",
      "epoch 77 | loss: 0.03555 | train_balanced_accuracy: 0.98592 | valid_balanced_accuracy: 0.98733 |  0:03:08s\n",
      "epoch 78 | loss: 0.03842 | train_balanced_accuracy: 0.98475 | valid_balanced_accuracy: 0.98564 |  0:03:10s\n",
      "epoch 79 | loss: 0.03934 | train_balanced_accuracy: 0.98677 | valid_balanced_accuracy: 0.9843  |  0:03:12s\n",
      "epoch 80 | loss: 0.03339 | train_balanced_accuracy: 0.98507 | valid_balanced_accuracy: 0.98261 |  0:03:15s\n",
      "epoch 81 | loss: 0.03146 | train_balanced_accuracy: 0.98797 | valid_balanced_accuracy: 0.9843  |  0:03:17s\n",
      "epoch 82 | loss: 0.03038 | train_balanced_accuracy: 0.98717 | valid_balanced_accuracy: 0.98323 |  0:03:20s\n",
      "epoch 83 | loss: 0.04418 | train_balanced_accuracy: 0.9869  | valid_balanced_accuracy: 0.98386 |  0:03:25s\n",
      "epoch 84 | loss: 0.03871 | train_balanced_accuracy: 0.98629 | valid_balanced_accuracy: 0.98519 |  0:03:28s\n",
      "epoch 85 | loss: 0.03287 | train_balanced_accuracy: 0.98834 | valid_balanced_accuracy: 0.98626 |  0:03:31s\n",
      "epoch 86 | loss: 0.031   | train_balanced_accuracy: 0.98611 | valid_balanced_accuracy: 0.98778 |  0:03:33s\n",
      "epoch 87 | loss: 0.03132 | train_balanced_accuracy: 0.9885  | valid_balanced_accuracy: 0.98537 |  0:03:35s\n",
      "epoch 88 | loss: 0.03352 | train_balanced_accuracy: 0.98837 | valid_balanced_accuracy: 0.98065 |  0:03:38s\n",
      "epoch 89 | loss: 0.03038 | train_balanced_accuracy: 0.98624 | valid_balanced_accuracy: 0.9843  |  0:03:40s\n",
      "epoch 90 | loss: 0.051   | train_balanced_accuracy: 0.98852 | valid_balanced_accuracy: 0.98323 |  0:03:41s\n",
      "epoch 91 | loss: 0.0319  | train_balanced_accuracy: 0.98786 | valid_balanced_accuracy: 0.98626 |  0:03:44s\n",
      "epoch 92 | loss: 0.03864 | train_balanced_accuracy: 0.98799 | valid_balanced_accuracy: 0.9843  |  0:03:45s\n",
      "epoch 93 | loss: 0.0369  | train_balanced_accuracy: 0.98903 | valid_balanced_accuracy: 0.9843  |  0:03:47s\n",
      "epoch 94 | loss: 0.02985 | train_balanced_accuracy: 0.98852 | valid_balanced_accuracy: 0.98626 |  0:03:49s\n",
      "epoch 95 | loss: 0.03115 | train_balanced_accuracy: 0.98972 | valid_balanced_accuracy: 0.98475 |  0:03:52s\n",
      "epoch 96 | loss: 0.0251  | train_balanced_accuracy: 0.99129 | valid_balanced_accuracy: 0.98323 |  0:03:55s\n",
      "epoch 97 | loss: 0.03174 | train_balanced_accuracy: 0.98887 | valid_balanced_accuracy: 0.98323 |  0:03:58s\n",
      "epoch 98 | loss: 0.04237 | train_balanced_accuracy: 0.98762 | valid_balanced_accuracy: 0.9843  |  0:04:02s\n",
      "epoch 99 | loss: 0.03815 | train_balanced_accuracy: 0.98969 | valid_balanced_accuracy: 0.98279 |  0:04:05s\n",
      "epoch 100| loss: 0.03346 | train_balanced_accuracy: 0.98783 | valid_balanced_accuracy: 0.98475 |  0:04:07s\n",
      "epoch 101| loss: 0.02821 | train_balanced_accuracy: 0.98953 | valid_balanced_accuracy: 0.98279 |  0:04:09s\n",
      "epoch 102| loss: 0.04736 | train_balanced_accuracy: 0.99038 | valid_balanced_accuracy: 0.98279 |  0:04:12s\n",
      "epoch 103| loss: 0.0293  | train_balanced_accuracy: 0.98693 | valid_balanced_accuracy: 0.98065 |  0:04:14s\n",
      "epoch 104| loss: 0.03137 | train_balanced_accuracy: 0.9907  | valid_balanced_accuracy: 0.98234 |  0:04:17s\n",
      "epoch 105| loss: 0.03481 | train_balanced_accuracy: 0.99195 | valid_balanced_accuracy: 0.98323 |  0:04:19s\n",
      "epoch 106| loss: 0.02924 | train_balanced_accuracy: 0.98937 | valid_balanced_accuracy: 0.98279 |  0:04:22s\n",
      "epoch 107| loss: 0.03129 | train_balanced_accuracy: 0.98932 | valid_balanced_accuracy: 0.97824 |  0:04:24s\n",
      "epoch 108| loss: 0.03712 | train_balanced_accuracy: 0.98818 | valid_balanced_accuracy: 0.98475 |  0:04:27s\n",
      "epoch 109| loss: 0.03597 | train_balanced_accuracy: 0.98879 | valid_balanced_accuracy: 0.98279 |  0:04:30s\n",
      "epoch 110| loss: 0.03628 | train_balanced_accuracy: 0.98935 | valid_balanced_accuracy: 0.98279 |  0:04:32s\n",
      "epoch 111| loss: 0.02754 | train_balanced_accuracy: 0.98799 | valid_balanced_accuracy: 0.98519 |  0:04:34s\n",
      "epoch 112| loss: 0.03842 | train_balanced_accuracy: 0.99076 | valid_balanced_accuracy: 0.98475 |  0:04:36s\n",
      "epoch 113| loss: 0.02889 | train_balanced_accuracy: 0.98855 | valid_balanced_accuracy: 0.98519 |  0:04:39s\n",
      "epoch 114| loss: 0.03226 | train_balanced_accuracy: 0.98988 | valid_balanced_accuracy: 0.98689 |  0:04:41s\n",
      "epoch 115| loss: 0.03844 | train_balanced_accuracy: 0.98908 | valid_balanced_accuracy: 0.98519 |  0:04:43s\n",
      "epoch 116| loss: 0.02965 | train_balanced_accuracy: 0.98959 | valid_balanced_accuracy: 0.98475 |  0:04:45s\n",
      "epoch 117| loss: 0.0339  | train_balanced_accuracy: 0.992   | valid_balanced_accuracy: 0.9843  |  0:04:48s\n",
      "epoch 118| loss: 0.02241 | train_balanced_accuracy: 0.98786 | valid_balanced_accuracy: 0.98475 |  0:04:51s\n",
      "epoch 119| loss: 0.03068 | train_balanced_accuracy: 0.98871 | valid_balanced_accuracy: 0.98323 |  0:04:53s\n",
      "epoch 120| loss: 0.0325  | train_balanced_accuracy: 0.99028 | valid_balanced_accuracy: 0.98475 |  0:04:54s\n",
      "epoch 121| loss: 0.0313  | train_balanced_accuracy: 0.99044 | valid_balanced_accuracy: 0.98323 |  0:04:57s\n",
      "epoch 122| loss: 0.02977 | train_balanced_accuracy: 0.98922 | valid_balanced_accuracy: 0.98582 |  0:04:59s\n",
      "epoch 123| loss: 0.03943 | train_balanced_accuracy: 0.99004 | valid_balanced_accuracy: 0.98386 |  0:05:02s\n",
      "epoch 124| loss: 0.03205 | train_balanced_accuracy: 0.99089 | valid_balanced_accuracy: 0.98234 |  0:05:04s\n",
      "epoch 125| loss: 0.03459 | train_balanced_accuracy: 0.98714 | valid_balanced_accuracy: 0.98109 |  0:05:06s\n",
      "epoch 126| loss: 0.0345  | train_balanced_accuracy: 0.98611 | valid_balanced_accuracy: 0.98323 |  0:05:08s\n",
      "epoch 127| loss: 0.02228 | train_balanced_accuracy: 0.98919 | valid_balanced_accuracy: 0.98279 |  0:05:10s\n",
      "epoch 128| loss: 0.04579 | train_balanced_accuracy: 0.98953 | valid_balanced_accuracy: 0.98234 |  0:05:13s\n",
      "epoch 129| loss: 0.04607 | train_balanced_accuracy: 0.98959 | valid_balanced_accuracy: 0.98519 |  0:05:15s\n",
      "epoch 130| loss: 0.03785 | train_balanced_accuracy: 0.99115 | valid_balanced_accuracy: 0.98368 |  0:05:17s\n",
      "epoch 131| loss: 0.03356 | train_balanced_accuracy: 0.99198 | valid_balanced_accuracy: 0.98582 |  0:05:18s\n",
      "epoch 132| loss: 0.02536 | train_balanced_accuracy: 0.98993 | valid_balanced_accuracy: 0.98475 |  0:05:20s\n",
      "epoch 133| loss: 0.03284 | train_balanced_accuracy: 0.99046 | valid_balanced_accuracy: 0.98323 |  0:05:22s\n",
      "epoch 134| loss: 0.03236 | train_balanced_accuracy: 0.992   | valid_balanced_accuracy: 0.98127 |  0:05:24s\n",
      "epoch 135| loss: 0.03223 | train_balanced_accuracy: 0.99091 | valid_balanced_accuracy: 0.98537 |  0:05:25s\n",
      "epoch 136| loss: 0.02867 | train_balanced_accuracy: 0.98991 | valid_balanced_accuracy: 0.98582 |  0:05:27s\n",
      "\n",
      "Early stopping occurred at epoch 136 with best_epoch = 86 and best_valid_balanced_accuracy = 0.98778\n",
      "Best weights from best epoch are automatically used!\n",
      "0.966935394538613\n"
     ]
    }
   ],
   "source": [
    "tbn_clf = TabNetClassifier(n_a=56, n_d=56, n_shared=1, n_steps=1, gamma=1, optimizer_fn=torch.optim.Adam,\n",
    "                       optimizer_params=dict(lr=2e-2),\n",
    "                       scheduler_params={\"step_size\":10, \n",
    "                                         \"gamma\":0.9},\n",
    "                       scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                       mask_type='entmax')\n",
    "tbn_clf.fit(X_train2, y_train,            \n",
    "    eval_set=[(X_train2, y_train), (x_val2, y_val)],\n",
    "    eval_name=['train', 'valid'],\n",
    "    eval_metric=['balanced_accuracy'],\n",
    "    max_epochs=1000 , patience=50,\n",
    "    batch_size=256, virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    weights=1,\n",
    "    drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.966935394538613\n"
     ]
    }
   ],
   "source": [
    "y_pred_tbn = tbn_clf.predict(x_test2)\n",
    "corr_tbn = matthews_corrcoef(y_pred_tbn, y_test)\n",
    "print(corr_tbn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, False, False,  True, False,  True, False,\n",
       "        True,  True, False, False, False,  True, False,  True, False,\n",
       "       False, False, False,  True,  True, False, False,  True,  True,\n",
       "        True,  True, False,  True,  True, False, False, False, False,\n",
       "       False,  True, False, False,  True,  True, False, False,  True,\n",
       "        True,  True,  True, False, False, False, False, False, False,\n",
       "        True,  True,  True,  True,  True, False, False, False, False,\n",
       "       False,  True, False, False,  True, False,  True, False, False,\n",
       "        True,  True, False, False, False,  True,  True, False, False,\n",
       "        True,  True, False,  True, False, False, False, False, False,\n",
       "        True, False, False, False, False,  True,  True,  True, False,\n",
       "       False, False,  True, False, False,  True,  True, False, False,\n",
       "       False, False,  True, False, False, False,  True, False, False,\n",
       "        True,  True,  True, False,  True, False,  True, False, False,\n",
       "       False, False,  True, False,  True,  True,  True,  True, False,\n",
       "        True, False,  True, False, False,  True, False, False, False,\n",
       "        True,  True,  True, False, False, False, False,  True, False,\n",
       "        True,  True, False, False,  True,  True, False,  True,  True,\n",
       "       False,  True,  True, False, False,  True,  True,  True,  True,\n",
       "       False, False, False,  True, False, False, False,  True,  True,\n",
       "        True,  True,  True,  True, False, False,  True, False, False,\n",
       "       False,  True, False,  True,  True,  True, False,  True, False,\n",
       "       False, False,  True,  True, False, False,  True,  True, False,\n",
       "       False, False,  True,  True,  True, False, False, False,  True,\n",
       "        True,  True, False, False, False, False,  True, False,  True,\n",
       "        True,  True,  True, False, False, False, False, False, False,\n",
       "       False, False,  True, False,  True,  True, False, False,  True,\n",
       "       False,  True,  True,  True,  True, False, False, False, False,\n",
       "       False,  True, False, False, False,  True,  True, False, False,\n",
       "        True,  True, False, False,  True,  True, False, False,  True,\n",
       "       False,  True, False, False, False, False,  True, False, False,\n",
       "        True,  True,  True, False, False, False, False, False,  True,\n",
       "       False, False, False,  True,  True, False,  True,  True,  True,\n",
       "        True, False,  True,  True, False, False, False, False, False,\n",
       "        True, False,  True, False,  True,  True,  True, False, False,\n",
       "        True, False,  True,  True,  True, False,  True, False, False,\n",
       "        True,  True, False,  True, False, False,  True, False, False,\n",
       "       False, False, False, False,  True,  True, False,  True, False,\n",
       "       False, False, False,  True, False, False,  True,  True,  True,\n",
       "       False,  True, False, False, False,  True, False, False, False,\n",
       "        True, False, False, False,  True, False, False,  True,  True,\n",
       "       False, False,  True,  True, False, False,  True,  True,  True,\n",
       "       False, False, False,  True,  True,  True, False, False, False,\n",
       "        True,  True, False,  True, False, False,  True, False,  True,\n",
       "       False,  True, False, False,  True,  True, False, False, False,\n",
       "       False, False,  True, False, False, False, False, False,  True,\n",
       "       False, False,  True, False,  True,  True, False,  True, False,\n",
       "        True, False, False,  True, False, False,  True, False, False,\n",
       "       False, False, False, False,  True, False, False, False,  True,\n",
       "       False,  True,  True, False, False, False, False, False,  True,\n",
       "       False, False, False, False, False,  True, False, False,  True,\n",
       "       False,  True, False, False, False,  True,  True, False,  True,\n",
       "       False, False,  True, False, False, False, False,  True,  True,\n",
       "       False,  True,  True, False, False,  True,  True, False, False,\n",
       "       False,  True,  True,  True, False,  True,  True,  True, False,\n",
       "       False, False,  True, False, False,  True,  True, False, False,\n",
       "        True, False, False,  True, False, False, False,  True, False,\n",
       "       False, False, False, False,  True, False, False, False, False,\n",
       "       False,  True, False, False,  True, False,  True, False,  True,\n",
       "        True,  True,  True, False, False,  True,  True,  True, False,\n",
       "       False, False,  True, False, False, False,  True,  True,  True,\n",
       "       False, False,  True,  True, False,  True, False, False, False,\n",
       "       False, False, False,  True,  True,  True,  True, False,  True,\n",
       "       False,  True, False,  True, False, False, False, False, False,\n",
       "       False,  True,  True,  True, False, False, False, False, False,\n",
       "        True, False, False, False,  True,  True, False,  True, False,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "        True, False, False, False, False, False, False, False, False,\n",
       "        True,  True,  True,  True,  True,  True, False, False, False,\n",
       "       False, False, False,  True, False, False,  True,  True, False,\n",
       "       False,  True, False,  True, False, False, False, False, False,\n",
       "        True,  True,  True,  True, False,  True,  True, False, False,\n",
       "        True, False, False, False, False, False, False,  True,  True,\n",
       "       False,  True,  True,  True, False, False,  True, False,  True,\n",
       "        True, False,  True, False, False,  True, False, False, False,\n",
       "       False, False, False,  True,  True,  True, False,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "       False, False,  True, False, False, False, False, False,  True,\n",
       "       False,  True, False, False, False, False, False, False,  True,\n",
       "        True,  True, False, False, False,  True, False, False, False,\n",
       "        True, False, False, False, False, False, False,  True, False,\n",
       "       False,  True, False, False,  True,  True,  True, False, False,\n",
       "        True, False,  True, False, False,  True,  True,  True, False,\n",
       "        True,  True, False, False, False,  True, False,  True, False,\n",
       "       False, False, False,  True, False,  True,  True,  True,  True,\n",
       "       False, False,  True, False, False, False,  True, False, False,\n",
       "        True, False, False, False, False, False, False, False,  True,\n",
       "        True, False,  True, False,  True, False, False,  True,  True,\n",
       "        True,  True, False,  True, False])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_metric(x, \n",
    "                     y,\n",
    "                     samples_cnt = 100,\n",
    "                     alpha = 0.05,\n",
    "                     random_state = 42):\n",
    "    size = len(x)\n",
    "    np.random.seed(random_state)\n",
    "    b_metric = np.zeros(samples_cnt)\n",
    "    for it in range(samples_cnt):\n",
    "        poses = np.random.choice(x.shape[0], size=x.shape[0], replace=True)\n",
    "        \n",
    "        x_boot = x[poses]\n",
    "        y_boot = y[poses]\n",
    "        \n",
    "        #print(x_boot, y_boot)\n",
    "\n",
    "        m_val = matthews_corrcoef(x_boot, y_boot)\n",
    "        b_metric[it] = m_val\n",
    "    \n",
    "    return b_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "boot_score_rf = bootstrap_metric(np.array(y_test), y_pred_rf)\n",
    "boot_score_xgb = bootstrap_metric(np.array(y_test), y_pred_xgb)\n",
    "boot_score_svc = bootstrap_metric(np.array(y_test), y_pred_svc)\n",
    "boot_score_tbn = bootstrap_metric(np.array(y_test), y_pred_tbn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "sns.boxplot(y=np.concatenate([boot_score_rf, \n",
    "                              boot_score_xgb, \n",
    "                              boot_score_svc,\n",
    "                              boot_score_tbn\n",
    "                             ]),\n",
    "            x=['RF'] * 1000 + ['XGBoost'] * 1000 + ['SVC'] * 1000 + [\"TabNet\"] * 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3cd8953005b4d594074671208b7f4922223c493d5ec38b9cea332931373e0c48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
